---
title: "P8451 Homework 7"
author: "Will Simmons"
date: "03/10/2020"
output: word_document
editor_options: 
  chunk_output_type: console
always_allow_html: true
---

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(NHANES)
library(caret)
library(pROC)
library(e1071)
library(randomForest)
```

# 1. Clean the data, as demonstrated in class. This includes removing ? as a missingness indicator, collapsing the heart disease present variable into “Present” and “Not Present”.

```{r}
miss_data <- read.csv("./data/processed.cleveland.data", header = FALSE)

var.names <- c("age", "sex", "pain_type", "resting_sysbp", "chol", "fast_blsugar_gt120", "rest_ecg", "max_hr", "exerc_angina", "ST_depression", "ST_slope", "vessels_colorflu", "defect", "heart_disease_present")

colnames(miss_data) <- var.names
str(miss_data)

miss_data[miss_data == "?"] <- NA

miss_data$defect <- as.numeric(factor(miss_data$defect))
miss_data$vessels_colorflu <- as.numeric(factor(miss_data$vessels_colorflu))

miss_data$outcome <- ifelse(miss_data$heart_disease_present == 0, 0,1)
miss_data$heart_disease_present <- NULL
miss_data$outcome <- factor(miss_data$outcome)
levels(miss_data$outcome) <- c("HD Not Present", "HD Present")
str(miss_data)
summary(miss_data)

#Remove the missings
data <- na.omit(miss_data)

#Set No Heart Disease as Reference Level
data$outcome <- relevel(data$outcome, ref = "HD Not Present")
#Clean up
rm(var.names)

## Changing var formats
data =
  data %>% 
  mutate(
    sex = as.factor(sex),
    pain_type = as.factor(pain_type),
    fast_blsugar_gt120 = as.factor(fast_blsugar_gt120), 
    rest_ecg = as.factor(rest_ecg),
    exerc_angina = as.factor(exerc_angina),
    ST_slope = as.factor(ST_slope),
    vessels_colorflu = as.factor(vessels_colorflu),
    defect = as.factor(defect)
  )
```

# 2. Run a single classification tree using all of the features available in the dataset. Calculate evaluation metrics and output the variable importance metrics.

First, I'll split the data into training and test sets.

```{r}

set.seed(1)

train_idx =
  createDataPartition(data$outcome,
                      p = 0.7,
                      list = FALSE)

train = 
  data[train_idx, ]

test = 
  data[-train_idx, ]

```

I need to check whether the data are approximately balanced with respect to the outcome. Otherwise, our predictive performance will suffer.

```{r}

train %>% 
  mutate(outcome2 = as.numeric(outcome) - 1) %>% 
  summarize(outcome_prevalence = mean(outcome2)) %>% knitr::kable()

```

Since the outcome prevalence is near 0.5, we don't have to worry as much about unbalanced data.

Now, I'll run a classification tree using all of the features available in the dataset.

```{r}
set.seed(1)

train_control =
  trainControl(method = "cv", number = 10)

tune_grid =
  expand.grid(
    cp = 10^seq(-5, -1.5, length = 100)
  )

single_tree =
  train(outcome ~ .,
        data = train,
        method = "rpart",
        trControl = train_control,
        tuneGrid = tune_grid)

# single_tree =
#   train(x = train[, -14],
#         y = train[, 14],
#         method = "rpart",
#         trControl = train_control,
#         tuneGrid = tune_grid)

```

As we can see from the plot below and from calling `bestTune` on the calculated model, our best `cp` value is **`r single_tree$bestTune`**.

```{r}

plot(single_tree)

single_tree$bestTune

```

Finally, we'll look at the characteristics of our final classification tree, including **accuracy** and **important variables**. We'll also look at a confusion matrix of how the model has classified the training data, and the final tree visualization. 

### Important variables - tree-based model
```{r final_tree_imp}

varImp(single_tree$finalModel) %>% 
  rownames_to_column(var = "Variable") %>% 
  arrange(desc(Overall)) %>% 
  rename(Importance = Overall) %>% 
  knitr::kable()

```

NOTE: I've used dummy variable coding to identify which levels of categorical variables are most important for prediction. This is opposed to keeping factor variables unified. You can see the most important variables listed above.

### Cross-validated accuracy - tree-based model

```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

get_best_result(single_tree) %>% 
  knitr::kable()

```

As we can see, our final tree model's accuracy is **`r get_best_result(single_tree)[[2]]`**.

### Confusion matrix - classification tree

```{r tree_confusion}

tree_pred = predict(single_tree, newdata = train[, -14])
table(tree_pred, train$outcome)

```

### Final tree visualization - classification tree

```{r}

rpart.plot::rpart.plot(single_tree$finalModel)

```

# 3. Use random forest to classify heart disease. Set up a pipeline to try different values of mtry and different numbers of trees to obtain your optimal model. Again, calculate appropriate evaluation metrics and output the variable importance metrics.

I'll use `purrr::map2()` to map over 2 sets of parameters iteratively within a random forest model. I'll then create a dataframe of the resulting pairs and their accuracies.

```{r}
set.seed(1)

## using model.matrix to dummify factor vars - randomForest doesn't do this automatically

x_forest <- model.matrix(outcome~.,data)[train_idx,-1] ## -1 removes intercept created using model.matrix
y_forest <- data$outcome[train_idx]

forest = function(m, n) {
  
  set.seed(1)
  
  ## formula implementation
  # a =
  #   randomForest(outcome ~.,
  #                data = train,
  #                mtry = m,
  #                ntree = n,
  #                importance = TRUE)
  
  ## model.matrix implementation
  a =
    randomForest(x = x_forest,
                 y = y_forest,
                 #data = train,
                 mtry = m,
                 ntree = n,
                 importance = TRUE)
  
  
  tibble(
    mtry = m,
    ntree = n,
    acc = 1 - a$err.rate[,1][n]  ## accuracy (1-err) for the nth model - shows accuracy for the full number of trees input

  )
  
}

## test
# f = forest(m = 1, n = 600)
# plot(1 - f$err.rate[,1])
# 
#   tibble(
#     mtry = 1,
#     ntree = 600,
#     acc = 1 - f$err.rate[,1][600]  ## accuracy (1-err) for the nth model - shows accuracy for the full number of trees input
# 
#   )

mtry_range = 1:13
ntree_range = seq(100, 2000, length = 20)

grid = 
  list(m = mtry_range, 
       n = ntree_range) %>% 
  cross_df()

rf_accuracies =
  map2(.x = grid$m, .y = grid$n, forest) %>% 
  bind_rows() %>% 
  arrange(desc(acc))
```

It looks like our top 6 combinations all have an approximate accuracy of `r rf_accuracies[1,3]`. We'll use the top result, where `mtry` = `r rf_accuracies[[1,1]]`, `ntree` = `r rf_accuracies[[1,2]]`.

```{r}

rf_accuracies %>%
  head(10) %>%
  knitr::kable()

```

Using these tuning parameters, I'll calculate appropriate evaluation metrics and output the variable importance metrics.

### Important variables - random forest

```{r}

set.seed(1)

final_forest =
  randomForest(x = x_forest, 
               y = y_forest, 
               mtry = as.numeric(rf_accuracies[1,1]), 
               ntree = as.numeric(rf_accuracies[1,2]),
               importance = TRUE)

varImpPlot(final_forest, class = TRUE)

rows = importance(final_forest) %>% rownames()

importance(final_forest) %>% 
  as_tibble() %>% 
  mutate("Variable" = rows) %>% 
  select(Variable, MeanDecreaseAccuracy) %>% 
  arrange(desc(MeanDecreaseAccuracy)) %>% 
  knitr::kable(caption = "Random Forest Model") %>% 
  kableExtra::kable_styling(full_width = F)

```

Since we're not interested in separating subpopulations, it makes most sense for us to look at the mean decrease in accuracy for each variable upon removing it from the model (`MeanDecreaseAccuracy`) as a measure of variable importance. It looks like `exerc_angina1` (having category 1 for the variable), `pain_type4` (having category 4 for the variable), and `defect3` (having category 3 for the variable) are the top three most important using this measure in our final random forest model.

NOTE: As above, I've used dummy variable coding to identify which levels of categorical variables are most important for prediction. This is opposed to keeping factor variables unified.

### OOB accuracy - random forest

I'll assess the out-of-bag (OOB) accuracy for the random forest. This is similar to the cross-validated accuracy, which should ideally be used to compare accuracy across models - but this will serve as an acceptable approximation.

```{r}

1 - final_forest$err.rate[as.numeric(rf_accuracies[1,2])]

```

The OOB accuracy for our final random forest model is `r 1 - final_forest$err.rate[as.numeric(rf_accuracies[1,2])]`. This is compared to the single tree's accuracy measure of `r get_best_result(single_tree)[[2]]`, which is slightly worse than our random forest model.

# 4. Answer the questions:

### a. Are there differences in variable importance that you see between a single tree and an ensemble metric?

Yes, there are slight differences in important variables between the single tree and the ensemble metric, but overall the variable categories are similar. (See NOTE above - I am ranking the importance of categorical levels of such variables.)

Let's compare:

```{r}

## rpart kable
varImp(single_tree$finalModel) %>% 
  rownames_to_column(var = "Variable") %>% 
  arrange(desc(Overall)) %>% 
  rename(Importance = Overall) %>% 
  knitr::kable(caption = "Single Tree") %>% 
  kableExtra::kable_styling(full_width = F,
                            position = "float_left")

## rf kable
importance(final_forest) %>% 
  as_tibble() %>% 
  mutate("Variable" = rows) %>% 
  select(Variable, MeanDecreaseAccuracy) %>% 
  arrange(desc(MeanDecreaseAccuracy)) %>% 
  knitr::kable(caption = "Random Forest") %>% 
  kableExtra::kable_styling(full_width = F,
                          position = "float_right")

```

### b. Are there differences observed across the different variable importance metrics output from the ensemble?

Since they're not displayed above, let's look at the two measures from the random tree:

```{r}

importance(final_forest) %>% 
  as_tibble() %>% 
  mutate("Variable" = rows) %>% 
  select(Variable, MeanDecreaseAccuracy, MeanDecreaseGini) %>% 
  arrange(desc(MeanDecreaseAccuracy)) %>% 
  knitr::kable(caption = "Random Forest") %>% 
  kableExtra::kable_styling(full_width = F)

```

Yes, there are differences in which variables are ranked as important when comparing the two measures, mean decrease in accuracy and mean decrease in Gini coefficient. I explain further below (Section c.ii.).

### c. How do you interpret those differences?

#### i. Differences between single-tree and random forest importances 

These models' respective variable importance measures are not measuring the same thing: the single tree's variable importance is a measure of goodness of fit in both primary variable splits and surrogates, and the random forest's is a measure of the mean decrease in accuracy across trees when a variable is removed. Thus, some amount of difference is due to this baseline difference. As an implication, single-tree importance is a variable's importance across splits of one tree, but random forest importance is measured globally across many trees: perhaps a more robust indicator of a variable's actual importance given the resampling nature of random forests.

#### ii. Differences across variable importance metrics from the ensemble

As we can see, there are fairly significant differences between which variables are determined important using mean decrease in accuracy and using mean decrease in Gini. As discussed before, mean decrease in accuracy gives a more interpretable measure of global importance across all trees in the forest, whereas Gini gives an indicator of how important a given variable may be in determining subpopulations via its splits - thus, differences in these measures may be due to this difference in interpretation. Which we use ultimately depends on how we want to define 'importance'.

# 5. Use a boosting algorithm and tune to obtain your optimal model. Compare to the results from the single classification tree and the random forest.

```{r}

set.seed(1)

gbm_grid =
  expand.grid(
    n.trees = seq(500, 5000, length = 10),
    shrinkage = 10^seq(-5, -1, length = 10),
    interaction.depth = 1,   ## default value but have to specify in tuneGrid
    n.minobsinnode = 10      ## default ...
  )

boost_model =
  train %>% 
  # mutate(
  #   outcome = as.numeric(outcome)   ## gbm package requires numeric outcome
  # ) %>% 
  # gbm(outcome ~.,
  #     data = .,
  #     distribution = 'bernoulli',
  #     n.trees = 2000,
  #     shrinkage = 0.1)
  train(outcome~., 
        data = .,
        distribution = 'bernoulli',
        method = 'gbm',
        trControl = train_control,
        tuneGrid = gbm_grid,
        verbose = FALSE           ## omg outputs a million lines of results if you don't do this
        )


boost_model$bestTune

#cheer::cheer()

```

After tuning two parameters - `n.trees` and `shrinkage` - we found the best values were `r boost_model$bestTune[[1]]` and `r boost_model$bestTune[[2]]`, respectively. (I left the other two parameters required for the `gbm` model at their defaults.)

I'll plot the loss function of the model using these parameters to make sure our model is optimized.

```{r}
library(gbm)
set.seed(1)

final_gbm =
  train %>% 
  mutate(
    outcome = as.numeric(outcome) - 1   ## gbm package requires numeric outcome
  ) %>%
  gbm(outcome ~.,
    data = .,
    distribution = 'bernoulli',
    n.trees = 1500,
    shrinkage = 4.641589e-03)
  

gbm.perf(final_gbm, plot.it=TRUE, oobag.curve=TRUE, overlay=TRUE, method='OOB')

```


### Important variables - gradient boosting model

The variables of importance are fairly similar to our previous results - for example, the top six variables here mirror ours from the random forest model, but in a slightly different order - but, as expected, neither the importance order nor values are exactly the same.

```{r}

summary(boost_model$finalModel) %>% 
  as_tibble() %>% 
  knitr::kable(caption = "Gradient Boosted Model") %>% 
  kableExtra::kable_styling(full_width = F)

```

### Accuracy - gradient boosting model

```{r}
set.seed(1)
pred_gbm = predict(final_gbm, train, n.trees = 1500, type = "response")
## need many more trees with boosting - growing stumps, so not informative

pred_gbm_class = round(pred_gbm)

misClasificError <- mean(pred_gbm_class != as.numeric(train$outcome) - 1)
print(paste('Accuracy Model',1 - misClasificError))

```

We can see that the accuracy of our GBM model is `r 1-misClasificError`, better than our previous models.

# 6. Which model performs the best? Provide justification for your answer.

The GBM model performs the best. We can see the error rates of the three models - single tree, random forest, and gradient boosted - below.

```{r}

## create kable of accuracies

tibble(
  "Model" = 
    c("Single Tree",
    "Random Forest",
    "GBM"),
  "Accuracy" =
  c(get_best_result(single_tree)[[2]], 
    1 - final_forest$err.rate[as.numeric(rf_accuracies[1,2])],
    1 - misClasificError)
  ) %>% 
  knitr::kable(caption = "Accuracies") %>% 
  kableExtra::kable_styling(full_width = F)


```

To do a final test of this, we can compare our test error rates for each of our models:

```{r}
## random forest

x_forest_test <- model.matrix(outcome~.,data)[-train_idx,-1] ## -1 removes intercept created using model.matrix
y_forest_test <- data$outcome[-train_idx]


randomForest(x = x_forest_test,
             y = y_forest_test,
             mtry = 4,
             ntree = 300,
             importance = TRUE)
  
  
  # tibble(
  #   mtry = m,
  #   ntree = n,
  #   acc = 1 - a$err.rate[,1][n]  ## accuracy (1-err) for the nth model - shows accuracy for the full number of trees input
  # 
  # )

## test
# f = forest(m = 1, n = 600)
# plot(1 - f$err.rate[,1])
# 
#   tibble(
#     mtry = 1,
#     ntree = 600,
#     acc = 1 - f$err.rate[,1][600]  ## accuracy (1-err) for the nth model - shows accuracy for the full number of trees input
# 
#   )

mtry_range = 1:13
ntree_range = seq(100, 2000, length = 20)

grid = 
  list(m = mtry_range, 
       n = ntree_range) %>% 
  cross_df()

rf_accuracies =
  map2(.x = grid$m, .y = grid$n, forest) %>% 
  bind_rows() %>% 
  arrange(desc(acc))


## GBM

set.seed(1)
pred_gbm = predict(final_gbm, test, n.trees = 1500, type = "response")
## need many more trees with boosting - growing stumps, so not informative

pred_gbm_class = round(pred_gbm)

misClasificError <- mean(pred_gbm_class != as.numeric(test$outcome) - 1)
print(paste('Accuracy Model',1 - misClasificError))

```


# 7. How do these results compare to the SVC analysis we did back in Class 6?

While the SVC model outperforms both the single tree and random forest models in terms of predictive accuracy, it does not outperform (or, depending on `set.seed`, likely performs comparably to) our boosted model.
